===========================================================================================================
===>>> Почему иногда на сигмоиде, когда мы застреваем веса меняют направление (-)
(см. в тетради гипотезу) + проверить:
СНАЧАЛА ПОДКЛЮЧИТЬ:
- сделать проект мавеновским (+)
- log4j чтобы можно было писать отладочную инфу (+)

после кажой эпохи для каждого веса строить график зависимости общей ошибки от каждого веса в его ближайшей окресности
так же выводить сообщение о смене знака дельты этого веса
Хочеться подтвердить теорию
Если она подтверждается то как тогда будет работать алгоритм Rprop ?
Ответ см. Caution: на стр. 95.

См. pendulum.xls
Комментарий:
Наблюдаем за любым Err(wi) - видим что до того момента когда мы обучились - график после каждой эпохи Err(wi - Delta <-> wi + Delta) при фиксации остальных весов
монотонный и не меняеться между эпохами, а потом начинается МАЯТНИК + нарастание ошибки происходит из-за погрешности вычислений в маятнике, т.к. нужно останавливатся в
этом моменте уже.
Вопрос - почему же маятник? (сделать функцию Err = f(wi, wj) и посмотреть если зависимость смены знака wi в области значений wj и если есть - дать сигнал) (-)
Вопрос - почему же сеть обучилась в
25-Dec-2017 16:18:05,001 MSK DEBUG - | Epoch: 20022; full relative error: 0.016706874999149142; current error: 0.002031510581508746; min error: 0.002031510581508746; TREND: decent;
Хотя если в маятнике посмотреть значений ошибки - они достигают меньшего результата: 3.581693	"0.001597" (?) - может локальный миниум


ОБНАРУЖЕНО:
Увеличил точность вывода цифр и теперь видно что даже в маятнике происходит постепенное уменьшение минимальной ошибки.
Т.е. она то последовательно в цикле n растёт, в цикле n-1 - обновляет минимальное значение, не правда не сильно существенно!


===========================================================================================================
===>>> Почему всё-таки мы застреваем в сигмоиде в 90% случаях, а в апроксимации гиперболического тангенса не застреваем вообще (-)
+ оказывается и в тангенсе тож застреваем в 50% случаев!!!!
- возможно не хватало возможностей нейронной сети, т.к. для такой сети:

                .inputsNeurons(2)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .addHiddenLevel(20)
                .outputNeurons(3)

Получаем довольно быстро такие результаты:
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  0.0000000000| => | -0.0021645008|| -0.0012672232|| -0.0000541552|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  1.0000000000| => |  0.0013704977||  0.0010928063||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  2.0000000000| => | -0.0003879812||  0.9601600000|| -0.0014055131|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  3.0000000000| => |  0.0180363532||  0.9601600000||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  4.0000000000| => |  0.9601600000|| -0.0009982358||  0.0157488933|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  5.0000000000| => |  0.9601600000||  0.0123391168||  0.9584562254|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  6.0000000000| => |  0.9601600000||  0.9601600000|| -0.0012663641|
15-Jan-2018 20:33:46,335 MSK DEBUG - Input: |  0.0000000000||  7.0000000000| => |  0.9601600000||  0.9601600000||  0.9601600000|
15-Jan-2018 20:33:46,335 MSK DEBUG - New minimum has been reached: 0.009962525386993621


===========================================================================================================
===>>> Изменить подход остановки обучения (-)
Если нет проверочных сэмплов:
- например может гонять определённое время и обновлять минимальное значение + запоминать в нём топологию
	спустя это время выдавать топологию по минимальному значению

Если есть проверочные сэмплы:
- прогнать проверочные сэмплы и напрмер оценить % провальных, например если у нас используется граничный фильтр на выходе каждого нейрона - пропускаем
	через неё и если хоть один выход отличается то результат провальный

Нужно останавливаться, когда за заданное кол-во циклов у нас не происходит обнавление миниума ошибок или же оно совсем несущественное, меньше заданной дельты


============================================================================================================
===>>> Новый вид NN, способный различать капчи, по структуре схож с мозгом
https://www.vicarious.com/2017/10/26/common-sense-cortex-and-captcha/
https://github.com/vicariousinc/science_rcn
Так же код лежит на Я.ДИСК тут: Common Sense, Cortex, and CAPTCHA
Можно покурить, понять идею, реализовать на java например

















